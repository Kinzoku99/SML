{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f5a820a"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?export=view&id=12CrUdXDAiltLBT26sG7HZ_HciIhvGyT8'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical machine learning - Notebook 2\n",
    "**Author: Michał Ciach, Grzegorz Preibisch, Dorota Celińska-Kopczyńska**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et5_2Neh7Sb1"
   },
   "source": [
    "In today's class, we will learn some aspects of parameter and interval estimation.\n",
    "In the first section, we will focus on the graphical analysis of the properties of the point estimator (e.g., mean value of a distribution) using artificial data.\n",
    "In the second section, we will focus on estimating the mean value of a distribution using a statistical sample.   \n",
    "In the last section, we'll calculate confidence intervals for the mean using real-world data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c1qctHRoY_V",
    "outputId": "d8217daa-18cc-441b-daed-b6670a1c7a64"
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PA1wjZ4w_ovO",
    "outputId": "a856bf85-9ef5-4816-d98e-d92d33401fb6"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpa_rkDoWlo"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UWS4VSq39w7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import plotly.express as px\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as tstud\n",
    "from typing import List\n",
    "from typing import Callable\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Dh5IunLi7M0x",
    "outputId": "2c3e032f-af67-4942-bcc1-06ec11f7597c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "protein_lengths = pd.read_csv('protein_lengths.tsv', sep='\\t')\n",
    "protein_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdIQlMXd2bly"
   },
   "source": [
    "## Point estimation -- simulational approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y14PycwLx8lX"
   },
   "source": [
    "During the lecture, we introduced the properties of the estimator. To develop more intuition about them, in this notebook we will graphically analyze their distributions (and the distributions of some relevant statistics).\n",
    "\n",
    "Contrary to other notebooks, in this one we will start with analyzing artificial data. The advantage of (pseudo)randomly generated data is that we are able to control nearly every aspect of the study. E.g., if we want to analyze the properties of the estimator, we need to know its true value (the groundtruth, something we know we should obtain). In real-world data we rarely know the true values. Using (pseudo)randomly generated data allows us to set the true values as the parameters for the generation mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP49P_Y2x8lY"
   },
   "source": [
    "**Exercise 1.** In this exercise we will analyze the distribution of a few estimators (for normally distributed data) from repeated experiments (the number of repetitions will be $N$). The size of the sample ($n$) will be the same in each experiment. To this end:\n",
    "\n",
    "* Write a function that will return $N$ samples of size $n$ from from gaussian distribution with a given mean $\\mu$ and variance $\\sigma^2$. Run the function with parameters: $N=1000$, $n = 30$, $\\mu = 200$, $\\sigma^2=144$\n",
    "* Based on the dataset from the previous point, for each sample of size $n$ compute the following estimators:\n",
    "\n",
    " - mean $\\hat{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$\n",
    " - unbiased estimator for variance $\\tilde{S}_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - biased estimator for variance $S_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - estimator for variance with mininal MSE $S_n^{2*} = \\frac{1}{n+1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    "\n",
    "Hint: For that purpose, we suggest to write a function and change the functions for the estimators when necessary. For variance estimators, make the fullest use of `np.var`.\n",
    "\n",
    "* Create the histograms for the values of the computed estimators. Plot the values of the true parameters ($\\mu = 200$, $\\sigma^2=144$ ). What can you say about the distribution of the estimators? Are there differences in the shape of the distributions of the estimators? Discuss and justify your view.\n",
    "* Compute the biases, variances, and MSE of the estimators. Do the results agree with the theoretical results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqK9EWoVx8la"
   },
   "outputs": [],
   "source": [
    "true_mu = 200\n",
    "true_stddev = 12\n",
    "true_var = 144\n",
    "size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3_14jrgx8lb"
   },
   "outputs": [],
   "source": [
    "def make_dataset(statistic : Callable[[np.array], float] = np.mean, N : int = 1000, sample_size : int = 3, mu : float = 0, std_dev : float = 1) -> List[np.array]:\n",
    "  \"\"\"\n",
    "  Function which samples N times from gaussian distribution with mean and std_dev\n",
    "  :param N: number of samples\n",
    "  :param sample_size: number of observations in one sample\n",
    "  :return: dataset of N samples of size n from normal distrution\n",
    "  \"\"\"\n",
    "  samples = []\n",
    "  for step in range(N):\n",
    "    sample = np.random.normal(mu,std_dev,sample_size)\n",
    "    samples.append(sample)\n",
    "  return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elJpGEELx8lb"
   },
   "outputs": [],
   "source": [
    "data = make_dataset(mu=true_mu, std_dev=true_stddev, sample_size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FDV2m1cx8lc"
   },
   "outputs": [],
   "source": [
    "def run_experiment( samples: List[np.array], statistic : Callable[[np.array], float] = np.mean  ) -> List[float]:\n",
    "  \"\"\"\n",
    "  Function which computes estimation for a given batch of data\n",
    "  :param samples: results of function make_dataset\n",
    "  :param statistic: function to estimate the parameters\n",
    "  :return: list of estimated parameters\n",
    "  \"\"\"\n",
    "  results = []\n",
    "  for sample in samples:\n",
    "    results.append(statistic(sample))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dnu_RScsx8le"
   },
   "outputs": [],
   "source": [
    "def var_unbiased(sample : np.array) -> float:\n",
    "  return np.var(sample,  ddof=1)\n",
    "\n",
    "def var_ML(sample : np.array) -> float:\n",
    "  return np.var(sample,  ddof=0)\n",
    "\n",
    "# it would be nicer to add an argument with the sample size\n",
    "def var_minMSE(sample : np.array) -> float:\n",
    "  return np.var(sample,  ddof=0)*(size/(size+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rKMGcPOx8le"
   },
   "outputs": [],
   "source": [
    "biased = run_experiment(samples = data, statistic = np.var)\n",
    "unbiased = run_experiment(samples = data, statistic = var_unbiased)\n",
    "minMSE = run_experiment(samples = data, statistic = var_minMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "zSaYYSgGx8le",
    "outputId": "fc104cce-0029-43f5-de06-398eba677ccd"
   },
   "outputs": [],
   "source": [
    "# code for variance\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=biased,  name='biased' ))\n",
    "fig.add_trace(go.Histogram(x=unbiased,  name='unbiased'))\n",
    "fig.add_trace(go.Histogram(x=minMSE,  name='minMSE'))\n",
    "# Overlay the histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.add_vline(x=true_var)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Y2hudv5nx8lf",
    "outputId": "03319a09-9c96-4021-a2e9-f0e4b074063f"
   },
   "outputs": [],
   "source": [
    "# code for mean\n",
    "results_mean = run_experiment(samples = data, statistic = np.mean)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=results_mean,  name='mean' ))\n",
    "# Reduce opacity to see histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.add_vline(x=true_mu)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTsDSVk8x8lf",
    "outputId": "f0185119-28bd-4b1d-ad90-8f59516d190b"
   },
   "outputs": [],
   "source": [
    "# code for bias\n",
    "bias_mean = np.mean(np.array(results_mean)) - true_mu\n",
    "print(bias_mean)\n",
    "bias_unbiased = np.mean(np.array(unbiased)) - true_var\n",
    "print(bias_unbiased)\n",
    "bias_biased = np.mean(np.array(biased)) - true_var\n",
    "print(bias_biased)\n",
    "bias_minMSE = np.mean(np.array(minMSE)) - true_var\n",
    "print(bias_minMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRz_vg-izMpy",
    "outputId": "ee5e504f-6372-4847-e98e-5d22a66bf99b"
   },
   "outputs": [],
   "source": [
    "# code for MSE\n",
    "MSE_mean = np.mean((true_mu - np.array(results_mean))**2)\n",
    "print(MSE_mean)\n",
    "MSE_unbiased = np.mean((true_var - np.array(unbiased))**2)\n",
    "print(MSE_unbiased)\n",
    "MSE_biased = np.mean((true_var - np.array(biased))**2)\n",
    "print(MSE_biased)\n",
    "MSE_minMSE = np.mean((true_var - np.array(minMSE))**2)\n",
    "print(MSE_minMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBSbfzZtzSmt"
   },
   "source": [
    "**Exercise 2. (optional)**  In the previous exercise, we worked with the distribution of the estimators by running multiple experiments with a given sample size (by default quite small). In this exercise, we will analyze the asymptotic properties of the estimators. To this end:\n",
    "\n",
    "* Compute 100 samples of each size from 2 to 5000 (you may keep using data from N(200,144)). For each sample compute:\n",
    "\n",
    " - $\\hat{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$ as a mean estimator\n",
    " - $\\hat{X} +10$ as a mean estimator\n",
    " - unbiased estimator for variance $\\tilde{S}_n^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    " - biased estimator for variance $S_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i-\\hat{X})^2$\n",
    "\n",
    "* Scatterplot the obtained values of the parameters against the sample size: y axis should the the value of the parameter, and x axis should be the size of the sample. Add a horizontal line with the true value of the parameter. Compare the plots. Do values of the estimators become closer to real values when the size of the sample increases? Do the results agree with the results on consistency of the estimators? Discuss.\n",
    "\n",
    "* Compute the biases of the estimators. Plot the biases of the estimators against the sample size. Add a horizontal line in zero. Compare the insights from the plots with the theory of asymptotical unbiasedness.\n",
    "\n",
    "Hint: We encourage you to use scatterplots, but visualizations of distributions may also come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_biased(sample : np.array) -> float:\n",
    "    return np.mean(sample) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample_size = 5000\n",
    "# For faster computations; should be set to 1 for exact solution\n",
    "step = 100\n",
    "\n",
    "sample_sizes = list(range(2,max_sample_size+1,step))\n",
    "\n",
    "mean_unb = []\n",
    "mean_b = []\n",
    "var_unb = []\n",
    "var_b = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    \n",
    "    samples = make_dataset(mu=true_mu, std_dev=true_stddev, N=100, sample_size=sample_size)\n",
    "\n",
    "    mean_unb.append([np.mean(sample) for sample in samples])\n",
    "    mean_b.append([mean_biased(sample) for sample in samples])\n",
    "    var_unb.append([var_unbiased(sample) for sample in samples])\n",
    "    var_b.append([var_ML(sample) for sample in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "fig = px.scatter(x=list(repeat(2,100)), y=mean_unb[0], title='Estimated unbiased mean')\n",
    "for ind in range(1,len(sample_sizes)):\n",
    "    fig.add_trace(go.Scatter(x=list(repeat(sample_sizes[ind],100)), y=mean_unb[ind], mode='markers',\n",
    "                             marker=dict(color=fig.data[0].marker.color)))\n",
    "\n",
    "fig.add_hline(y=true_mu)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=list(repeat(2,100)), y=mean_b[0], title='Estimated biased mean')\n",
    "for ind in range(1,len(sample_sizes)):\n",
    "    fig.add_trace(go.Scatter(x=list(repeat(sample_sizes[ind],100)), y=mean_b[ind], mode='markers',\n",
    "                            marker=dict(color=fig.data[0].marker.color)))\n",
    "\n",
    "fig.add_hline(y=true_mu)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=list(repeat(2,100)), y=var_unb[0], title='Estimated unbiased variance')\n",
    "for ind in range(1,len(sample_sizes)):\n",
    "    fig.add_trace(go.Scatter(x=list(repeat(sample_sizes[ind],100)), y=var_unb[ind], mode='markers',\n",
    "                            marker=dict(color=fig.data[0].marker.color)))\n",
    "\n",
    "fig.add_hline(y=true_var)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=list(repeat(2,100)), y=var_b[0], title='Estimated biased variance')\n",
    "for ind in range(1,len(sample_sizes)):\n",
    "    fig.add_trace(go.Scatter(x=list(repeat(sample_sizes[ind],100)), y=var_b[ind], mode='markers',\n",
    "                            marker=dict(color=fig.data[0].marker.color)))\n",
    "\n",
    "fig.add_hline(y=true_var)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mean_unb = [np.mean(estimations) - true_mu for estimations in mean_unb]\n",
    "fig = px.scatter(x=sample_sizes, y=bias_mean_unb, title='Bias of the unbiased estimator of the mean.')\n",
    "fig.add_hline(y=0)\n",
    "fig.show()\n",
    "\n",
    "bias_mean_b = [np.mean(estimations) - true_mu for estimations in mean_b]\n",
    "fig = px.scatter(x=sample_sizes, y=bias_mean_b, title='Bias of the biased estimator of the mean.')\n",
    "fig.add_hline(y=0)\n",
    "fig.show()\n",
    "\n",
    "bias_var_unb = [np.mean(estimations) - true_var for estimations in var_unb]\n",
    "fig = px.scatter(x=sample_sizes, y=bias_var_unb, title='Bias of the unbiased estimator of the variance.')\n",
    "fig.add_hline(y=0)\n",
    "fig.show()\n",
    "\n",
    "# The difference can be better seen with sample sizes in the range [2,100].\n",
    "bias_var_b = [np.mean(estimations) - true_var for estimations in var_b]\n",
    "fig = px.scatter(x=sample_sizes, y=bias_var_b, title='Bias of the biased estimator of the variance.')\n",
    "fig.add_hline(y=0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7bLoYLTz9CH"
   },
   "source": [
    "**Exercise 3 -- optional homework.** Find an MLE estimator for $\\lambda$ in exponential distribution (probability density function  $f(x) = \\lambda \\exp(-\\lambda x)$ for $x \\in [0, \\infty)$), using pen and paper.\n",
    "Prepare a similar analysis of the properties of the found estimator as the analyses in Exercise 1 and 2 (analysis of the distribution with a given sample size and the analysis of the asymptotical properties based on the increasing sample sizes). You may assume true $\\lambda = 2$. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4zsbLpx8lg"
   },
   "source": [
    "## Exploratory analysis\n",
    "The first step to any statistical analysis is to explore the data - check the basic statistics like the mean and variance, and visualize the data to see what kind of distribution we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPeeEe5tvyOB"
   },
   "source": [
    "**Exercise 4.** In this exercise, we'll extract the data about human proteins, perform a simple data transformation, and do a basic exploratory analysis.\n",
    "\n",
    "1. Select the data about human protein lengths from the `protein_lengths` data frame, and put it into a data frame `human_protein_lengths`. Here, you may need to use the `.copy()` method for the subsequent steps to work (ask your tutor if you need a further explanation).\n",
    "2. Calculate the base-10 logarithm of the protein length and append it to the `human_protein_lengths` data frame as a column called `LogLength`.\n",
    "3. Use the `human_protein_lengths.describe()` method to check the basic statistics of the numerical columns of the data frame. What is the average length of a human protein? What is the maximum length?  \n",
    "4. Use the `px.histogram()` functions to create histograms showing the distributions of the protein lengths and log-lengths. Which distribution is more spread around its average? Does any distribution resemble the Normal (Gaussian) distribution? Are there many proteins with lengths similar to the maximum length, or just a few?  \n",
    "5. Calculate the average length and log-length and their standard deviations; Store them in variables `true_mean`, `true_mean_log`, `true_std` and `true_std_log`. We'll use them in subsequent exercises as our *ground truth* against which we'll evaluate our estimators.     \n",
    "\n",
    "*Quick question*. Is $\\text{true_mean}$ equal to $10^\\text{true_mean_log}$? Why/why not? (note that we've used the base-10 logarithm)\n",
    "\n",
    "*Why the base-10 logarithm?* Mathematicians usually prefer to use the natural logarithms. In statistical data analysis, we sometimes use also the base-10 logarithms, because their values are easier to interpret as orders of magnitude (or simply the numbers of digits) of our values. Although the logarithms are mostly equivalent mathematically, an easier interpretation is important to get more meaningful conclusions from the data.\n",
    "\n",
    "*Why the standard deviation?* Some students may wonder why we prefer the standard deviation rather than the variance - after all, the difference is just a square root, so mathematically it's almost the same thing. Here, again, the reason is the interpretability of the results. When we compute the variance, we square the observations. As a consequence, their units also get squared. This means that, if we estimate the number of mushrooms in a forest, the variance is expressed in terms of *mushrooms squared*, which doesn't make any sense. Taking a square root brings the unit back to mushrooms.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D9OcfeRowf8T",
    "outputId": "3a533a20-f616-4dc5-af42-56d2cdd163e9"
   },
   "outputs": [],
   "source": [
    "human_protein_lengths = protein_lengths.loc[protein_lengths['Common name'] == 'Human'].copy()\n",
    "# Note: without .copy(), some versions of Pandas may return a View.\n",
    "# This may interfere with adding a new column to human_protein_lengths.\n",
    "print(human_protein_lengths.head())\n",
    "\n",
    "human_protein_lengths['LogLength'] = np.log10(human_protein_lengths['Protein length'])\n",
    "\n",
    "print(human_protein_lengths.describe())\n",
    "\n",
    "fig = px.histogram(x=human_protein_lengths['Protein length'], title=\"Distribution of the human protein lengths\")\n",
    "fig.show()\n",
    "fig = px.histogram(x=human_protein_lengths['LogLength'], title=\"Distribution of the human protein log-lengths\")\n",
    "fig.show()\n",
    "\n",
    "true_mean = human_protein_lengths['Protein length'].mean()\n",
    "true_mean_log = human_protein_lengths['LogLength'].mean()\n",
    "true_std = human_protein_lengths['Protein length'].std()\n",
    "true_std_log = human_protein_lengths['LogLength'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmcN4rsybZqV"
   },
   "source": [
    "## Point estimation\n",
    "\n",
    "One of the main strengths of statistical theory is that it allows us to estimate many quantities (like the mean protein length, mean income in a country, or voting preferences)  using only a sample of randomly selected observations, and, most importantly, to estimate the uncertainty of such estimation. This is the main reason why we derive properties of estimators, such as their expected value and variance. Good statisticians can derive estimators which need less observations and give better results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjVsjUljgcXy"
   },
   "source": [
    "**Exercise 5.** In this exercise, we'll do an empirical analysis of the properties of the estimator of the mean. We'll use a sample of $N=1000$ randomly selected human proteins. Denote $X_i$ as the length of a randomly selected human protein, and $\\log(X_i)$ as its base-10 logarithm. Define the following two estimators:\n",
    "\n",
    "$$\\hat{\\mu}_X = \\sum_{i=1}^N X_i/N, \\text{an estimator of the mean length}$$\n",
    "\n",
    "$$\\hat{\\mu}_{\\log(X)} = \\sum_{i=1}^N \\log(X_i)/N, \\text{an estimator of the mean log-length}$$  \n",
    "\n",
    "First, we'll draw $R=2000$ independent samples and calculate the estimators. Here's an example way to do this:   \n",
    "\n",
    "1. Create empty lists called e.g. `means` and `means_log`.  \n",
    "2. Repeat the following $R$ times (e.g. using a `for` loop):  \n",
    "    2.1. Get a random sample of size $N$ of the observations (i.e. rows) from `human_protein_lengths`; you can use the `.sample()` method.   \n",
    "    2.2. Calculate the mean length and append to `means`  \n",
    "    2.3. Calculate the mean log-length and append to `means_log`.     \n",
    "3. Convert both lists to `numpy` arrays (e.g. `means = np.array(means)`)\n",
    "\n",
    "Now, we can inspect how well the estimators approximate the *true* mean length $\\mu_X$ and the *true* mean log-length $\\mu_{\\log{X}}$ (notice the lack of hats above $\\mu$'s - this means that these are the true parameters, not estimators).\n",
    "\n",
    "4. Estimate the mean value of the estimator of the mean (by running `np.mean(means)`). Is it close to the true value $\\mu$? In other words, does the estimator seem *unbiased*?\n",
    "5. Estimate the bias of the estimator of the mean log-length (using the values in `means_log`). Does it seem biased? Does the result agree with the theoretical one about the estimator of the mean?       \n",
    "5. Estimate the Root Mean Square Error of the estimator of the mean, defined as $\\text{RMSE}(\\hat{\\mu}_X) = \\sqrt{\\mathbb{E}(\\hat{\\mu}_X - \\mu_X)^2}$. This will tell you, approximately, the average error of $\\hat{\\mu}_X$ in terms of the number of amino acids (the building blocks of proteins). Why did I write *approximately*? (Hint: it's not just becasue we estimate it rather than calculate it theoretically)\n",
    "6. Estimate $\\text{RMSE}(\\hat{\\mu}_{\\log(X)})$. How can you interpret the result?\n",
    "7. Estimate the standard deviations of the estimators. Which one is less variable? Does it mean that one quantity is easier to estimate than the other?\n",
    "8. Is $\\text{sd}(\\hat{\\mu}_X) = \\text{RMSE}(\\hat{\\mu}_X)$? Why/why not? Is the equation always true, sometimes true, or never true?    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93utUUg31uTs"
   },
   "outputs": [],
   "source": [
    "## Part 1\n",
    "N = 1000  # Sample size\n",
    "R = 1000  # Number of replications\n",
    "means = []\n",
    "means_log = []\n",
    "for i in range(R):\n",
    "  sample = human_protein_lengths.sample(N)\n",
    "  means.append(sample['Protein length'].mean())\n",
    "  means_log.append(sample['LogLength'].mean())\n",
    "means = np.array(means)\n",
    "means_log = np.array(means_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUImSUnR4JkF",
    "outputId": "902e4e45-2ddb-4727-e904-be1096a32862"
   },
   "outputs": [],
   "source": [
    "## Part 2\n",
    "print()\n",
    "print('Symmetry of the distributions of the estimators:')\n",
    "print('Fraction of estimates over the true mean of lengths:', np.mean(means > true_mean))\n",
    "print('Fraction of estimates over the true mean of log-lengths:', np.mean(means_log > true_mean_log))\n",
    "print()\n",
    "print('Means of the estimators:')\n",
    "print('Mean for mean length:', np.mean(means))\n",
    "print('Mean for mean log-length:', np.mean(means_log))\n",
    "print()\n",
    "print('Bias of the estimators:')\n",
    "print('Bias for mean length:', np.mean(means - true_mean))\n",
    "print('Bias for mean log-length:', np.mean(means_log - true_mean_log))\n",
    "print()\n",
    "print('RMSE of the estimators:')\n",
    "print('RMSE for mean length:', np.sqrt(np.mean((means - true_mean)**2)))\n",
    "print('RMSE for mean length:', np.sqrt(np.mean((means_log - true_mean_log)**2)))\n",
    "print()\n",
    "print('Variability of the estimators:')\n",
    "print('SD for mean length:', np.std(means))\n",
    "print('SD for mean log-length:', np.std(means_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQIElPvrKT7o"
   },
   "source": [
    "**Exercise 6.** The standard deviation of an unbiased estimator tells us how much it fluctuates around the true value. An estimator with a lower standard deviation will more often give us values that are close to the true one. This way, we can compare two estimators of the same thing.  \n",
    "\n",
    "However, the standard deviation is often not that useful when comparing the measurements of two different things. This is because it depends on the units of the measurement. Suppose we measure the length $L$ of some objects in meters, and the standard deviation of the measurement is $\\text{sd}(L)$. Measuring the same object in centimeters will give us a measurement equal $100L$, and the corresponding standard deviation $\\text{sd}(100L) = 100\\text{sd}(L)$ will appear to be much larger, but it doesn't mean that measurements in centimeters are more difficult. To make matters worse, in real-life applications, the variability of the measurement often depends on its average value, regardless of the units. The standard deviation of the height of a mouse (a few milimeters) is much lower than the one of an elephant (several centimeters), but it doesn't mean that mice are easier to measure. Similarly, in the case of protein length and log-length, the latter is much smaller, so it can be expected that its standard deviation will be smaller as well.\n",
    "\n",
    "To evaluate the variability of an estimator regardless of its units and the average value, we can calculate a so-called [*coefficient of variation*](https://en.wikipedia.org/wiki/Coefficient_of_variation) (variation, not variance!). For a random value $Y$, this is defined as $\\text{cv}(Y) = \\text{sd}(Y)/\\mathbb{E}(Y)$.   \n",
    "\n",
    "7. Calculate the coefficients of variation for the estimators of mean protein length, i.e. $\\text{cv}(\\hat{\\mu}_X)$, and log-length, i.e. $\\text{cv}(\\hat{\\mu}_{\\log(X)})$. Which estimator is better in this case? In general, does a lower coefficient of variation always mean that an estimator is better?\n",
    "8. Is $\\text{cv}(Y)$ always equal to  $\\text{sd}(Y/\\mathbb{E}(Y))$? Is there a condition for $Y$ that makes it equal? Give an analytical argument and verify that empirically on the protein length data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7aI1Wt739_w",
    "outputId": "2b855135-2b63-4c27-eedb-c336403e2a5c"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print('Coefficient of variation of the estimators:')\n",
    "print('CV for mean length:', np.std(means)/np.mean(means))\n",
    "print('CV for mean log-length:', np.std(means_log)/np.mean(means_log))\n",
    "print()\n",
    "print('Coefficient of variation of the estimators, method 2:')\n",
    "# Yup, it's that simple - we just change the location of one bracket.\n",
    "# But in more advanced applications it's worth to realize and remember this property of the std.\n",
    "print('CV for mean length:', np.std(means/np.mean(means)))\n",
    "print('CV for mean log-length:', np.std(means_log/np.mean(means_log)))\n",
    "print()\n",
    "print('Relative RMSE of the estimators:')\n",
    "print('Relative RMSE for mean length:', np.sqrt(np.mean((means - true_mean)**2))/true_mean)\n",
    "print('Relative RMSE for mean length:', np.sqrt(np.mean((means_log - true_mean_log)**2))/true_mean_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsrcrt2_SR5H"
   },
   "source": [
    "## Interval estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuDDXuOyqPKg"
   },
   "source": [
    "In the previous section, we've learned how to quantify and analyze the uncertainty of an estimation by analyzing the standard deviation of the estimator. In this section, we will learn a different technique - the estimation of *confidence intervals*, i.e. intervals which are likely to contain the true value of the parameter of interest.\n",
    "\n",
    "In general, we say that a confidence interval $[A, B]$ for a parameter $\\theta$ has a confidence level $\\alpha$ if it contains the true value of the parameter $\\theta$ with probability $\\alpha$:\n",
    "\n",
    "$$\\mathbb{P}(A \\leq \\theta \\leq B) = \\alpha$$\n",
    "\n",
    "Above, $A$ and $B$ are random variables calculated from the data (i.e. $A$ and $B$ are *statistics*). Note: some authors use a different terminology and would call this a level $1-\\alpha$; some authors also use a more general definition with $\\mathbb{P} \\geq \\alpha$ instead of $\\mathbb{P} = \\alpha$, because we often can't determine the exact probability and can only give its lower bound (you've seen this in the lecture with the Chebyshev confidence intervals).\n",
    "\n",
    "In principle, we can construct confidence intervals for any parameter of any distribution (e.g. the expected value, the variance, the proportion in the Bernoulli distribution, the shape parameter of the Gamma distribution etc.), but this is often difficult in practice. We'll focus on confidence intervals for the true mean (i.e. the expected value) of a normally distributed population - this is one of the most commonly used and one of the most useful confidence intervals.\n",
    "\n",
    "For the expected value of a normally distributed random variable, there are two commonly used confidence intervals: the conficence interval for a *known* $\\sigma$, given by the equation\n",
    "\n",
    "$$\\left (\\hat{\\mu} - q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}},\\quad \\hat{\\mu} + q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}} \\right ), $$\n",
    "where $q_{(1+\\alpha)/2}$ is the quantile of the standard normal distribution at the level of $(1+\\alpha)/2$; and the confidence interval for an *unknown* $\\sigma$, given by the equation\n",
    "\n",
    "$$\\left (\\hat{\\mu} - t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}},\\quad \\hat{\\mu} + t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}} \\right ), $$\n",
    "where $t{(1+\\alpha)/2, N-1}$ is the quantile of the Student's $t$ distribution with $N-1$ degrees of freedom at the level of $(1+\\alpha)/2$, and $\\hat{\\sigma}$ is the square root of the **unbiased** estimator of the variance, i.e. $\\hat{\\sigma} = \\sqrt{\\sum_{i=1}^N (X_i - \\bar{X})^2/(N-1)}$, where $\\bar{X} = \\hat{\\mu} = \\sum_{i=1}^N X_i$.\n",
    "\n",
    "If we simply plug $\\hat{\\sigma}$ instead of $\\sigma$ in the first kind of the confidence interval (the one for a known $\\sigma$), we get a third type of a confidence interval, a so-called *asymptotic confidence interval* for the mean; the name *asymptotic* comes from the fact that $\\hat{\\sigma} → \\sigma$ as $N → ∞$. As a consequence of this convergence, the asymptotic confidence interval gives quite accurate results for large sample sizes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtHFq07aZ2-f"
   },
   "source": [
    "**Exercise 10.** In this exercise, we'll do an empirical comparison of the properties of the three types of confidence intervals using the protein log-length data. We'll sample $R$ samples of some size $N$, calculate the corresponding confidence intervals for the mean, and check whether they have the desired confidence level and compare their lengths.   \n",
    "\n",
    "First, we'll prepare our data.    \n",
    "1. Create empty lists (or `numpy arrays`) that will contain the information whether the true mean is within a confidence interval (e.g. `within_normal` for the confidence interval with a known $\\sigma$, `within_student` for the confidence interval with an unknown $\\sigma$, `witin_asymptotic` for the asymptotic confidence interval).   \n",
    "2. Create empty lists (or `numpy arrays`) that will contain the lengths of the intervals.    \n",
    "3. Repeat the following $R=1000$ times (or more):  \n",
    "  3.1. Select a random sample of size $N$ of protein log-lengths; select $N$ of your choice.   \n",
    "  3.2. Calculate the three confidence intervals on the confidence level 95%. For the quantiles, you can use the `norm.ppf` and `t.ppf` functions from the `scipy.stats` package. For the normal confidence interval, use the known standard deviation in `true_mean_log`. Pay attention to the type of the estimator of standard deviation that you use! Some packages use the unbiased estimator of the variance, some don't!  \n",
    "  3.3. Calculate the lengths of the confidence intevals and append them to the corresponding lists.\n",
    "  3.4. Check whether the confidence intervals contain the true average log-length $\\mu_{\\log(X)}$, append the information to the corresponding lists.  \n",
    "\n",
    "Now, we'll use the generated data to analyze the properties of the confidence intervals.\n",
    "\n",
    "4. For each type of the confidence interval, estimate the probability that it contains $\\mu_{\\log(X)}$. Is the estimated probability close to the desired confidence level for each type? Why/why not? Does the answer depend on $N$?  \n",
    "5. Calculate the average length of each type of the confidence interval. Which type tends to give the shortest intervals? Which type tends to give the longest? Why?    \n",
    "6. Plot histograms depicting the distribution of the lengths of the confidence intervals.\n",
    "7. What are the advantages and disadvantages of each type of the confidence interval? Does the asymptotic confidence interval have any advantages over the other two?   \n",
    "8. *Quick question.* For a single sample, do all of the three types of confidence intervals always contain $\\hat{\\mu}_{\\log(X)}$?  \n",
    "\n",
    "Repeat this exericise using the protein lengths instead of log-lengths. What went wrong and why? Does the answer depend on $N$?      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HmqRibmsST0B",
    "outputId": "759e9399-ad02-4530-c03e-6ab5a91c3493"
   },
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "alpha = 0.95\n",
    "R = 2000\n",
    "N = 20\n",
    "sqrtN = np.sqrt(N)\n",
    "q = norm.ppf((1+alpha)/2)\n",
    "t = tstud.ppf((1+alpha)/2, N-1)\n",
    "\n",
    "# Generating the data\n",
    "normal_radius = q*true_std_log/sqrtN  # this is non-random; chage here to \"true std\" for protein lengths\n",
    "within_normal = []\n",
    "within_student = []\n",
    "within_asymptotic = []\n",
    "length_student = []\n",
    "length_asymptotic = []\n",
    "for _ in range(R):\n",
    "  sample = human_protein_lengths['LogLength'].sample(N)  # change here to 'Protein length' for protein lengths\n",
    "  mdiff = abs(sample.mean() - true_mean_log)  # change here to \"true mean\" for protein lengths\n",
    "  std_log = sample.std() # pandas uses the N-1 normalization\n",
    "\n",
    "  student_radius = t*std_log/sqrtN\n",
    "  asymptotic_radius = q*std_log/sqrtN\n",
    "\n",
    "  length_student.append(2*student_radius)\n",
    "  length_asymptotic.append(2*asymptotic_radius)\n",
    "\n",
    "  within_normal.append(mdiff <= normal_radius)\n",
    "  within_student.append(mdiff <= student_radius)\n",
    "  within_asymptotic.append(mdiff <= asymptotic_radius)\n",
    "\n",
    "# Analyze the confidence intervals\n",
    "print('Estimated confidence levels:')\n",
    "print('Gaussian confidence interval:', np.mean(within_normal))\n",
    "print('Student confidence interval:', np.mean(within_student))\n",
    "print('Asymptotic confidence interval:', np.mean(within_asymptotic))\n",
    "print()\n",
    "print('Average lengths:')\n",
    "print('Gaussian confidence interval:', 2*normal_radius)\n",
    "print('Student confidence interval:', np.mean(length_student))\n",
    "print('Asymptotic confidence interval:', np.mean(length_asymptotic))\n",
    "\n",
    "px.histogram(x=length_student, title=\"Student confidence interval length\").show()\n",
    "px.histogram(x=length_asymptotic, title=\"Asymptotic confidence interval length\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fSF9sIaU2O0"
   },
   "source": [
    "**Exercise 11 -- optional homework.** The length of the confidence interval is another (and more common) way to determine the required sample size. In this exercise, we'll see how to use it. As in exercise 6, we'll focus on the case of a known standard deviation.  \n",
    "\n",
    "1. Using the formula for the confidence interval with a known standard deviation, derive a formula for a necessary sample size $N^*$ such that the length of the confidence interval is at most some value $l$.  \n",
    "2. Calculate the required sample size for the average log-length for $l = 0.3$ (approximately 10% of the true mean).   \n",
    "3. Select a sample of the size calculated in the previous point and calculate an example confidence interval. Check if its length really is at most $l$.  \n",
    "  3.1. *Quick question 1.* Is the length of this confidence interval a random variable? Does it depend on the random sample?     \n",
    "4. *Quick question 2.* Do your calculations work for protein lengths as well?  \n",
    "5. Take a look at the formula for the confidence interval for an unknown standard deviation. Can you see why it may be difficult to derive a formula for $N^*$ in this case?  \n",
    "  5.1.\\* How would you approach calculating $N^*$ in this case?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer to 1): We need to solve $2q_{(1+\\alpha)/2} \\sigma/\\sqrt{N^*} = l$, this gives us $N^* = 4q_{(1+\\alpha)/2}^2\\sigma^2/l^2$.\n",
    "\n",
    "Answer to 3.1): No. The length is given by\n",
    "$$q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}},$$\n",
    "where $\\sigma$ is the true, known value of the standard deviation.\n",
    "\n",
    "Answer to 4): They don't work for small samples, which is the use case for this kind of equation, so it basically doesn't work at all.   \n",
    "\n",
    "Answer to 5): Because we'd need to invert the Student's t distribution quantile function.\n",
    "\n",
    "Answer to 5.1): This can be done numerically.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "alpha = 0.95\n",
    "q = norm.ppf((1+alpha)/2)\n",
    "## Get the number of samples so that the length of the confidence interval\n",
    "## is equal to 0.3, i.e., approx. 0.1 of the true mean:\n",
    "c = 0.3/true_mean_log\n",
    "N = math.ceil(4*q**2*true_std_log**2/(c**2*true_mean_log**2))\n",
    "print('We need', N, 'samples for a confidence interval of length at most', c*true_mean_log)\n",
    "sample = human_protein_lengths['LogLength'].sample(N)\n",
    "mu = np.mean(sample)\n",
    "s = np.std(sample)\n",
    "conf = [mu - q/np.sqrt(N)*true_std_log, mu + q/np.sqrt(N)*true_std_log]\n",
    "print('Example confidence interval from', N, 'samples:', conf)\n",
    "print('Actual length:', conf[1]-conf[0])  # this is independent\n",
    "conf_asymptotic = [mu - q/np.sqrt(N)*s, mu + q/np.sqrt(N)*s]\n",
    "print('Example asymptotic confidence interval from', N, 'samples:', conf_asymptotic)\n",
    "print('Length of the corresponding asymptotic confidence interval:', conf_asymptotic[1] - conf_asymptotic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH4SXcd-7LFH"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\">\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'>\n",
    "\n",
    "\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\".   \n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
