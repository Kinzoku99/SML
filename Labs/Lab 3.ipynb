{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOnAEqRq3TOc"
   },
   "source": [
    "## Description\n",
    "In today's class, we'll learn a method of statistical inference called *hypothesis testing*.   \n",
    "\n",
    "*Theory time.* The general idea of statistical hypothesis testing is to set up a statistical hypothesis - e.g., that some parameter is greater than 0 - and try to *reject it* by showing that our data would be very unlikely to observe if this hypothesis was true.     \n",
    "\n",
    "Why do we reject statistical hypotheses rather than confirm them? Because in general, confirming them is impossible using random samples. For example, let's suppose that we study the toxicity of a new drug by giving it to 10 rats and observing them for a week. Just because the rats didn't get sick may suggest that the drug is safe, but it doesn't prove it - after all, it's just 10 animals observed for a short time. On the other hand, if all the 10 animals get sick (and the control group is fine), we can definitely reject the hypothesis that the drug is safe.   \n",
    "\n",
    "Another example: let's suppose we have a hypothesis that the average log-length of a human protein is equal 2.30, and we sample 200 proteins to verify it. If their average log-length turns out to be similar to 2.30 - for example, 2.31 with a standard deviation 0.1 - this doesn't prove our hypothesis. The true average log-length may still be equal, for example, 2.29 or 2.305. However, if the average log-length of our sample turns out to be 2.70 with a standard deviation 0.1, this is a strong evidence that our hypothesis is false.  \n",
    "\n",
    "More generally: just because some data somewhat agree with our beliefs doesn't prove that our beliefs are true. However, if the data contradicts our beliefs, then it proves our beliefs are false.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwGqN5_Q4162"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7112,
     "status": "ok",
     "timestamp": 1696421225006,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "7c1qctHRoY_V",
    "outputId": "3274ad66-c86b-4184-b416-cd3501f9b486"
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5060,
     "status": "ok",
     "timestamp": 1696421230060,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "jS4Mwv37_lgM",
    "outputId": "8bbf08c6-e7d0-46b5-e54b-93188d7b5034"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0\n",
    "!gdown https://drive.google.com/uc?id=1y5NKR3aWB0DbAuSWcg6ffa1Atu2unpOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696421230060,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "1qclcOYOz3qg"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1696421232437,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "EkH7lfBv405O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.stats import t as tstud\n",
    "from scipy.stats import ttest_ind, ttest_rel, ttest_1samp, norm, kstest, mannwhitneyu, shapiro, chisquare, chi2_contingency\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1696421233149,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "QFOe8o1n41Ec",
    "outputId": "e6ba8473-9548-4ba1-e8f7-533dfab1b385"
   },
   "outputs": [],
   "source": [
    "protein_lengths = pd.read_csv('protein_lengths.tsv', sep='\\t')\n",
    "protein_lengths['LogLength'] = np.log10(protein_lengths['Protein length'])\n",
    "protein_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1696421233151,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "QVYnO3rm_sSc",
    "outputId": "55a02c04-5591-4d1a-987b-70800c838672"
   },
   "outputs": [],
   "source": [
    "human_protein_lengths = protein_lengths.loc[protein_lengths['Common name'] == 'Human'].copy()\n",
    "# Note: without .copy(), some versions of Pandas may return a View.\n",
    "# This may interfere with adding a new column to human_protein_lengths.\n",
    "print(human_protein_lengths.head())\n",
    "print()\n",
    "print(human_protein_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1696421233152,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "Fs8D4zlT1pTm",
    "outputId": "8d6e4d7a-a1b1-44c6-b2a2-3edf4fa0f7a4"
   },
   "outputs": [],
   "source": [
    "citizen_incomes = pd.read_csv('citizen incomes.tsv', sep='\\t')\n",
    "citizen_incomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student's t-test\n",
    "\n",
    "It is any statistical hypothesis test in which the test statistic follows a <a href=\"https://en.wikipedia.org/wiki/Student%27s_t-distribution\">Student's t-distribution</a> under the null hypothesis.\n",
    "\n",
    "##### Z-test\n",
    "\n",
    "We assume that $X_1, X_2, \\ldots, X_n$ are normally distributed with **unknown** mean $\\mu$ and **known** varinace $\\sigma^2$. We formulate the null hypothesis $H_0$ that the mean is $\\mu_0$, while the alternative hypothesis $H_1$ states that either $\\mu \\neq \\mu_0$ (two-sided) or $\\mu > \\mu_0$ (one-sided). The test statistics is as follows:\n",
    "\n",
    "$$Z = \\frac{\\bar{x}-\\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} = \\sqrt{n}\\frac{\\bar{x}-\\mu_0}{\\sigma}.$$\n",
    "\n",
    "Then $Z$ has the standard distribution, i.e., $Z \\sim N(0,1)$, which allows us to determine the critical region and to calculate the p-value. In the case of two-sided test, p-value is the probability under the null hypothesis that Z is \"more extreme\" than $|z|$. Based on the p-value, we reject the null hypothesis if\n",
    "\n",
    "$$P(Z \\leq -|z|) + P(Z \\geq |z|) = 2P(Z \\leq -|z|) \\leq \\alpha.$$\n",
    "\n",
    "#### One-sample t-test\n",
    "\n",
    "If the variance of the normally distributed population is **not known**, then for testing a hypothesis regarding the mean the one-sample t-test is used. Let $H_0 : \\mu = \\mu_0$ and $H_1: \\mu \\neq \\mu_0$. The test statistic is given by \n",
    "\n",
    "$$T = \\sqrt{n}\\frac{\\bar{x}-\\mu_0}{s},$$\n",
    "\n",
    "where $s$ is the sample standard deviation, i.e.,\n",
    "\n",
    "$$s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i-\\bar x)^2}.$$\n",
    "\n",
    "Then the test statistic T has the Student's t-distribution with $n-1$ degrees of freedom and thus the p-value can be calculated as follows:\n",
    "\n",
    "$$2P(T_{n-1} \\leq -|t|).$$\n",
    "\n",
    "\n",
    "#### Two-sample t-test\n",
    "\n",
    "This is a test of the null hypothesis that the means of two normally distributed populations are equal **with** the assumption that the **unknown** variances $\\sigma_x^2$ and $\\sigma_y^2$ of the two populations are **equal**.\n",
    "\n",
    "We consider two samples from the two distributions, i.e., $x_1, x_2, \\ldots, x_n$ and $y_1, y_2, \\ldots, y_m$. We test the hypothesis that $H_0: \\mu_x = \\mu_y$ against $H_1: \\mu_x \\neq \\mu_y$. Let $\\bar{x}$ and $\\bar{y}$ be the sample means of the two samples, respectively. Similarly, let us denote by $s_x^2$ and $s_y^2$ the variances of the two samples. Then, the test statistic is given by:\n",
    "\n",
    "$$t = \\frac{(\\bar{x} - \\bar{y}) - (\\mu_x - \\mu_y)}{S_p\\sqrt{\\frac{1}{n} + \\frac{1}{m}}},$$\n",
    "\n",
    "where the $S_p^2$ (pooled variance) is defined as follows:\n",
    "\n",
    "$$S_p^2 = \\frac{(n-1)s_x^2+(m-1)s_y^2}{n+m-2}.$$\n",
    "\n",
    "The t statistic follows the t-distribution with $n+m-2$ degrees of freedom.\n",
    "\n",
    "\n",
    "#### Two-sample t-test (Welch's test)\n",
    "\n",
    "This is a test of the null hypothesis that the means of two normally distributed populations are equal **without** the assumption that the variances of the two populations are equal. \n",
    "\n",
    "We consider two samples from the two distributions, i.e., $x_1, x_2, \\ldots, x_n$ and $y_1, y_2, \\ldots, y_m$. We test the hypothesis that $H_0: \\mu_x = \\mu_y$ against $H_1: \\mu_x \\neq \\mu_y$. Let $\\bar{x}$ and $\\bar{y}$ be the sample means of the two samples, respectively. Similarly, let us denote by $s_x^2$ and $s_y^2$ the variances of the two samples. Then, the test statistic is given by:\n",
    "\n",
    "$$t = \\frac{(\\bar{x} - \\bar{y}) - (\\mu_x - \\mu_y)}{\\sqrt{\\frac{s_x^2}{n} + \\frac{s_y^2}{m}}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZIUR3I848-T"
   },
   "source": [
    "## Testing the value of the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNuOAtPb-o_Q"
   },
   "source": [
    "To illustrate the basic concepts of statistical hypothesis testing, we'll start with simple tests for a hypothesis that the true mean value is equal to some value $\\mu_0$, with an alternative hypothesis that it's different:  \n",
    "\n",
    "$$H_0: \\mu = \\mu_0$$\n",
    "$$H_1: \\mu \\neq \\mu_0$$  \n",
    "Note the lack of hats above any of the symbols - the hypotheses are about parameters, not estimators or any other random values.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPsbtyP5_zEV"
   },
   "source": [
    "**Exercise 1.** Consider the human protein log-length data in the `human_protein_lengths` data frame. Consider the following two null hypotheses: $H_0^{(1)}: \\mu = 2.711540$ and $H_0^{(2)}: \\mu = 6$. We'll use the Student's t test to verify the hypotheses using a random sample.  \n",
    "\n",
    "1. Select a random sample of protein log-lengths of size $N=20$.   \n",
    "2. Calculate the test statistics for one-sample Student's t-tests with the assumption that the standard deviation is unknown and is estimated from the sample. Pay attention which kind of the variance estimator you need to use (biased or unbiased). How many test statistics do you need to calculate to test the two hypotheses, $H_0^{(1)}$ and $H_0^{(2)}$?   \n",
    "3. Use the `tstud.ppf` (i.e. the quantile function of Student's t distribution) to calculate the critical set on the significance level 5% (i.e., Type I error risk 5%). Pay attention to the shape of the critical set - for our alternative hypothesis $H_1$, this set is a union of two semi-lines. How many quantile values do you need to calculate to test the two hypotheses, $H_0^{(1)}$ and $H_0^{(2)}$?   \n",
    "  3.1. Based on the values of the test statistic and the critical set, do we reject our null hypotheses? Did we correctly detect which hypothesis is true and which is false?    \n",
    "4. Use the `tstud.cdf` to calculate the p-values. Again, pay attention to the shape of the critical set. How many cdf values do you need to calculate to test the two hypotheses, $H_0^{(1)}$ and $H_0^{(2)}$?  \n",
    "  4.1. Based on the p-values, do we reject any of our hypotheses on the significance level 5%? Did we correctly detect which hypothesis is true and which is false?     \n",
    "5. Compare your results to the Student's t test implementation in `scipy`. The appropriate test has already been loaded in the *Data & imports* section.   \n",
    "6. Are there any assumptions of the test that are violated? If so, how strongly and what effect could it have on the test result?  \n",
    "7. Based on the results of this exercise, can you conclude that $N=20$ is enough to prove that $\\mu=2.711540$? Does the answer to this question depend on $N$?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "mu1 = 2.711540\n",
    "mu2 = 6.\n",
    "N = 20\n",
    "df = N - 1\n",
    "alpha = 0.05\n",
    "\n",
    "# Get the sample\n",
    "sample=human_protein_lengths.sample(N)\n",
    "\n",
    "## Calculate the test statistic\n",
    "mean_log = sample['LogLength'].mean()\n",
    "print('Estimated mean:', mean_log)\n",
    "# Here we use pandas.Series.std with the default setting ddof=1, i.e., the unbiased standard deviation estimator.\n",
    "# With std() from NumPy, we would have to use std_log = np.std(sample['LogLength'].values, ddof=1), i.e., we would have\n",
    "# to explicitly set ddof to 1 because the default value is 0.\n",
    "std_log = sample['LogLength'].std()\n",
    "test1 = np.sqrt(N)*(mean_log - mu1)/std_log\n",
    "test2 = np.sqrt(N)*(mean_log - mu2)/std_log\n",
    "print('Value of the test statistic for H^1_0:', test1)\n",
    "print('Value of the test statistic for H^2_0:', test2)\n",
    "\n",
    "## Calculate the critical regions\n",
    "q = tstud.ppf(1 - alpha/2, df)\n",
    "print('We reject the hypothesis if T <', -q, \"or T >\", q)\n",
    "\n",
    "## Calculate the p-value:\n",
    "print('P-value for H^1_0:', 2*tstud.cdf(-abs(test1), df))\n",
    "print('P-value for H^2_0:', 2*tstud.cdf(-abs(test2), df))\n",
    "\n",
    "## Verification of the p-value and critical region calculation\n",
    "print('P-value corresponding to the critical region:', 2*tstud.cdf(-q, df))\n",
    "\n",
    "## Scipy implementation:\n",
    "print('Scipy results, H^1_0:', ttest_1samp(sample['LogLength'], popmean=mu1))\n",
    "print('Scipy results, H^2_0:', ttest_1samp(sample['LogLength'], popmean=mu2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WKP_F-NHsMl"
   },
   "source": [
    "**Exercise 2.** In this exercise, we'll see a more useful application of statistical hypothesis testing: comparing two populations. Say we want to use a random sample to check if the log-lengths of human proteins are, on average, higher than the ones of another organism - like the bay bolete (a kind of mushroom).\n",
    "\n",
    "1. What are the appropriate null and alternative hypotheses if we want to use a random sample to show that human proteins are longer in terms of the average log-length?     \n",
    "2. Select a random sample of human proteins ($N_\\text{human}=20$) and a random sample of bay bolete proteins ($N_\\text{bolete}=20$) from the `protein_lengths` data frame.\n",
    "3. Use the two-sample Student's t-test implemented in the `ttest_ind` function from the `scipy` library to calculate the p-value. Pay attention to the `alternative` keyword, as well as to any keyword arguments that may correspond to assumptions such as the equality of variances.   \n",
    "4. Based on the p-value, do we reject $H_0$ on a significance level 5%? Did we confirm that humans have longer or shorter proteins than the mushroom (in terms of the log-length)?      \n",
    "5.\\*\\* Implement your own test and compare the p-value. You can use the equations described [here](https://en.wikipedia.org/wiki/Welch%27s_t-test).  \n",
    "6. What happens if we take a reverse hypothesis - that humans have lower protein log-lengths than the mushroom? Did our results confirm anything now?  \n",
    "  6.1. Is the result of the test true? Compare the true average log-lengths of the two organisms.  \n",
    "  6.2. Is the result of the test from point 5 true?   \n",
    "7.\\* Do a test for a null hypothesis that the average protein log-lengths are equal, and an alternative that they are different (in any direction; a so-called *two-sided* alternative hypothesis). Explain the difference in the results compared to the previous points.    \n",
    "  7.1. Roughly speaking, what is the difference in the shape of the critical region for a one-sided and a two-sided alternative hypothesis?   \n",
    "  7.2.\\* Calculate the ratio of the p-value for the two-sided alternative to the p-value for the one-sided alternative that the human log-lengths are smaller than the bolete log-lengths. Explain the result.  \n",
    "8. Can we use the Student's t test to test protein lengths rather than log-lengths?   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "N_human = 20\n",
    "N_bolete = 20\n",
    "\n",
    "## Sample\n",
    "human_sample=protein_lengths.loc[protein_lengths['Common name'] == 'Human', 'LogLength'].sample(N_human)\n",
    "bolete_sample=protein_lengths.loc[protein_lengths['Common name'] == 'Bay bolete (mushroom)', 'LogLength'].sample(N_bolete)\n",
    "\n",
    "## Do the test:\n",
    "print('Scipy results for Human > Bolete alternative:', ttest_ind(human_sample, bolete_sample, alternative='less', equal_var=False))\n",
    "print('Scipy results for Human < Bolete alternative:', ttest_ind(human_sample, bolete_sample, alternative='greater', equal_var=False))\n",
    "print('Scipy results for a two-sided alternative:', ttest_ind(human_sample, bolete_sample, alternative='two-sided', equal_var=False))\n",
    "print('Two-sided p-value divided by the one-sided p-value:',\n",
    "      ttest_ind(human_sample,\n",
    "                bolete_sample,\n",
    "                alternative='two-sided')[1]/ttest_ind(human_sample,\n",
    "                                                      bolete_sample,\n",
    "                                                      alternative='greater')[1])\n",
    "# Note: ttest_ind(a, b, alternative='greater') tests for E(a) > E(b).\n",
    "# Warning: ttest_ind assumes equal variances by default!\n",
    "\n",
    "## Implement the test:\n",
    "meanX = human_sample.mean()\n",
    "meanY = bolete_sample.mean()\n",
    "var_meanX = human_sample.var()/N_human  # an estimator of the variance of the estimator of the mean of X\n",
    "var_meanY = bolete_sample.var()/N_bolete\n",
    "delta_mean = meanX-meanY\n",
    "sd_delta = np.sqrt(var_meanX + var_meanY)\n",
    "test_statistic = delta_mean / sd_delta\n",
    "print('Manually calculated value of the T statistic:', test_statistic)\n",
    "df = sd_delta**4 / (var_meanX**2/(N_human-1) + var_meanY**2/(N_bolete-1))\n",
    "print('Manually calculated p-value (one-sided):',  1 - tstud.cdf(abs(test_statistic), df))\n",
    "\n",
    "## Compare to the true averages:\n",
    "print(protein_lengths.groupby('Common name').mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric tests\n",
    "\n",
    "The two-sample Student's t-test assumes a normal distribution of the populations. When this assumption is only slightly violated, like for the protein log-length data, the results may still be reliable, especially for large sample sizes. For many data sets, as the sample size increases, the distribution of the estimator of the mean converges to the Normal one (this is guaranteed by the Central Limit Theorem). This means that the estimator of the mean is distributed \"more normally\" than the original data (where \"more normal\" refers to the distance between the cumulative distribution functions). This increases the robustness of the t-test for small deviations from normality. However, when this assumption is strongly violated, like for the non-transformed protein length data, the results are no longer reliable. One way to solve this problem is to use non-parametric tests. A non-parametric test is defined as a test that does not rely on the assumption of a distribution of the data.   \n",
    "\n",
    "One of the most common non-parametric tests is the Mann-Whitney U-test, also known as the two-sample Wilcoxon's test. It's often used as a replacement for the Student's t-test when the data is not distributed normally. However, the null hypotheses of these two tests are different, and it's important to understand this difference to avoid misleading results.\n",
    "\n",
    "In contrast to the Student's t-test, the Mann-Whitney's one doesn't test the equality of parameters like the mean - hence the name *non-parametric*. Instead, its null hypothesis is that $\\mathbb{P}(X > Y) = 1/2$, i.e., that if we take a random observation $X$ from the first sample, and a random observation $Y$ from the second sample, it's equally likely that the first is greater or smaller than the second. A one-sided alternative hypothesis may be, e.g., that  $\\mathbb{P}(X > Y) > 1/2$, i.e., that samples from the first population tend to be larger than sample from the second one. In this case, we say that the first sample is *stochastically greater* than the second one.  \n",
    "\n",
    "Sidenote: the actual null hypothesis of the Mann-Whitney test is slightly different, but the one described above is a very close approximation that's much simpler to interpret and use in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.\\*** Implement you own version of the Mann-Whitney's test. You can find the necessary equations [here](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) (use the normal approximation for the test statistic).\n",
    "\n",
    "1. Use your implementation to test whether the protein lengths are higher in human than in the bay bolete (use a random sample of size $N$ of your choice).\n",
    "2. Compare your results to the `mannwhitneyu` function from `scipy`. Pay attention to the default parameters to obtain identical results.  \n",
    "3. Compare the results of the Mann-Whitney's test to the Student's t  test. Are the results of the two tests consistent? Can you conclude that one of the organisms has longer proteins? Is this a correct result?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_human = 20\n",
    "N_bolete = 20\n",
    "\n",
    "bolete_protein_lengths = protein_lengths.loc[protein_lengths['Common name'] == 'Bay bolete (mushroom)']\n",
    "\n",
    "sample1 = protein_lengths.loc[protein_lengths['Common name'] == 'Human'].sample(N_human)\n",
    "sample2 = protein_lengths.loc[protein_lengths['Common name'] == 'Bay bolete (mushroom)'].sample(N_bolete)\n",
    "\n",
    "## Testing lengths\n",
    "print('Scipy test results for lengths:')\n",
    "print(ttest_ind(sample1['Protein length'],\n",
    "          sample2['Protein length'],\n",
    "          equal_var=False,\n",
    "          alternative='greater'))\n",
    "\n",
    "print(mannwhitneyu(sample1['Protein length'],\n",
    "             sample2['Protein length'],\n",
    "             alternative='greater'))\n",
    "# Note: mannwhitneyu(a, b, alternative='greater') checks if a is stochastically\n",
    "# greater than b (the null is that they're equal)\n",
    "\n",
    "## Manual implementation\n",
    "print()\n",
    "print('Manual test results for lengths:')\n",
    "# Combine the data:\n",
    "combined = pd.concat([sample1, sample2]).copy()\n",
    "# Get the ranks:\n",
    "combined.sort_values('Protein length', inplace=True)\n",
    "combined['rank'] = range(1, combined.shape[0]+1)\n",
    "# Compute the statistic:\n",
    "U = sum(combined.loc[combined['Common name'] == 'Human', 'rank'])\n",
    "U -= N_human*(N_human+1.)/2.\n",
    "print('Manually computed value of the U test statistic:', U)\n",
    "# U is now equal to the number of pairs of human and bolete proteins where\n",
    "# the human one is longer.\n",
    "# Therefore, small values correspond to the alternative hypothesis.\n",
    "# Compute the mean and standard deviation of the test statistic:\n",
    "mU = N_human*N_bolete/2.\n",
    "sdU = np.sqrt(N_human*N_bolete*(N_human+N_bolete+1.)/12.)\n",
    "Z = (U-mU)/sdU\n",
    "print('Expected value of U:', mU, 'with standard deviation:', sdU)\n",
    "print('Z-score:', Z)\n",
    "# Compute the approximate p-value in a one-sided test:\n",
    "pval = 1 - norm.cdf(Z)\n",
    "print('Manually computed p-value:', pval)\n",
    "print(mannwhitneyu(sample1['Protein length'],\n",
    "             sample2['Protein length'],\n",
    "             alternative='greater',\n",
    "             use_continuity=False,\n",
    "             method = 'asymptotic'))  # to make it equal to our statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** The `citizen_income` data frame, loaded in the *Data & modules* section, contains the information about the yearly income in USD of randomly sampled individuals from two countries, encoded as Country `A` and Country `B`. Use an appropriate statistical test to check whether citizens of one of the countries earn more than citizens of the other country. If you use more than one test and get contradictory results, explain why that happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the reverse direction of the hypotheses below\n",
    "print('T-test results:', ttest_ind(citizen_incomes.loc[citizen_incomes['Country']=='A', 'Income'],\n",
    "                                   citizen_incomes.loc[citizen_incomes['Country']=='B', 'Income'],\n",
    "                                   equal_var=False,\n",
    "                                   alternative='less'))\n",
    "print('U-test results:', mannwhitneyu(citizen_incomes.loc[citizen_incomes['Country']=='A', 'Income'],\n",
    "                                      citizen_incomes.loc[citizen_incomes['Country']=='B', 'Income'],\n",
    "                                      alternative='greater'))\n",
    "\n",
    "print('Average incomes:', citizen_incomes.groupby('Country').mean())\n",
    "print('Median incomes:', citizen_incomes.groupby('Country').median())\n",
    "\n",
    "px.histogram(citizen_incomes, x='Income', color='Country', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKF6lqq9iikE"
   },
   "source": [
    "**Exercise 5.\\*\\*** In this exercise, we'll see how the violations of test assumptions influence the distribution of the test statistics under the null hypothesis.   \n",
    "\n",
    "1. Formulate a statistical hypothesis about the incomes of citizens of country `B` which you can test using any version of the Student's t test (regardless whether its assumptions are satisfied), and in which the null hypothesis $H_0$ is *true*.   \n",
    "  1.1.\\* Optionally, formulate a hypothesis which you can test using both Student's t and Mann-Whitney's u test.  \n",
    "2. Which assumptions of your test are violated on this data set? Are they approximately satisfied for large sample sizes?    \n",
    "3. Repeat the following $R=5000$ times:   \n",
    "  3.1. Draw two random samples, each of size $N=10$.   \n",
    "  3.2. Calculate the values of the Student's T statistic, either manually or using functions from `scipy`.   \n",
    "  3.3.\\* Calculate the Mann-Whitney's U statistic using functions from `scipy`.\n",
    "  3.4. Save the values of the statistics in lists.  \n",
    "4. Create a histogram that depicts the distribution of the statistic. Is the distribution correct (i.e. do they agree with the theoretical asumptions of the tests)? If not, how does it influence the test results?   \n",
    "  4.1.\\* Draw the probability density function of the theoretical distribution of the test statistics (under the null hypothesis) on the histograms.  \n",
    "  4.2. Calculate the probability that your test makes a false positive error in this data set (i.e. that it incorrectly rejects a true null hypothesis; i.e. that the test statistic is within the theoretically calculated critical region).  \n",
    "5. Did we analyze all the possible ways in which violated assumptions can influence test results? If not, what other problems or errors can be caused by the violated assumptions?   \n",
    "6. What happens if you use protein log-lengths instead of citizen incomes?     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "N = 10\n",
    "R = 5000\n",
    "\n",
    "test_population = citizen_incomes.loc[citizen_incomes['Country']=='B', 'Income']\n",
    "# test_population = human_protein_lengths['LogLength']\n",
    "\n",
    "## Distributions of the test statistics\n",
    "T_values = []\n",
    "U_values = []\n",
    "\n",
    "for _ in range(R):\n",
    "  sample1 = test_population.sample(N)\n",
    "  sample2 = test_population.sample(N)\n",
    "  #sample1 = human_protein_lengths['Protein length'].sample(N)\n",
    "  #sample2 = human_protein_lengths['Protein length'].sample(N)\n",
    "  T = ttest_ind(sample1,\n",
    "                sample2,\n",
    "                equal_var=False)[0]\n",
    "  U = mannwhitneyu(sample1,\n",
    "                   sample2,\n",
    "                   method = 'asymptotic')[0]\n",
    "  # Note: the value of the test statistic doesn't depend on the alternative\n",
    "  # hypothesis; method = 'asymptotic' is faster\n",
    "  T_values.append(T)\n",
    "  U_values.append(U)\n",
    "\n",
    "\n",
    "fig1 = px.histogram(T_values, histnorm='probability density')\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=np.linspace(-3, 3), y=tstud.pdf(np.linspace(-3, 3), N-1),\n",
    "                name='Student\\'s T')\n",
    "\n",
    ")\n",
    "# Just for comparison, I add a normal pdf\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=np.linspace(-3, 3), y=norm.pdf(np.linspace(-3, 3)),\n",
    "    name='Gaussian')\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "# For the U statistic, remember about that its expectation isn't 0.\n",
    "# We can use the expectations from the previous exercise\n",
    "mU = N**2/2.\n",
    "sdU = np.sqrt(N**2*(2*N+1.)/12.)\n",
    "fig2 = px.histogram(U_values, histnorm='probability density')\n",
    "fig2.add_trace(\n",
    "    go.Scatter(x=np.linspace(mU-3*sdU, mU+3*sdU), y=norm.pdf(np.linspace(mU-3*sdU, mU+3*sdU), loc=mU, scale=sdU),\n",
    "    name='Gaussian')\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "# Calculate the empirical probability of the critical regions\n",
    "qt = tstud.ppf(1 - alpha/2, df)\n",
    "qn = norm.ppf(1-alpha/2)\n",
    "print('Probability of Type I error for Mann-Whitney:', np.mean([abs((u-mU)/sdU) > qn for u in U_values]))\n",
    "print('Probability of Type I error for Student:', np.mean([abs(t) > qt for t in T_values]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
